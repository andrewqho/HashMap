{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Addressing HashMaps\n",
    "\n",
    "I recently stumbled across the wild world of hashmaps when I was writing a chapter about how dictionaries are implemented in Python and fell down a pretty deep rabbit hole. Intruiged, I set out to determine whether I myself could write a hashmap faster that would rival or potentially even surpass the performance of *std::unordered_map*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Operations\n",
    "\n",
    "When implementing my hashmaps, I focused on 3 main functions:\n",
    "\n",
    "1. **void insert(K& const key, V& const value)**: Takes a key and a value and inserts a new pair into the hashmap\n",
    "\n",
    "2. **bool emplace(K const &key, V &value)**: Takes a key and a value variable and queries the hashmap. If the value does exist, the function writes the corresponding value to the given value variable and returns true. If the value doesn't exist, then the function returns false.\n",
    "\n",
    "3. **bool remove(K const &key)**: Takes a key and queries the hashmap. If the value exists, then the hashmap removes the entry and the function returns true. If the value doesn't exist, then the function returns false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worst-Case vs. Average-Case\n",
    "\n",
    "Hashmaps are known to have $O(N)$ worst case complexities, since it's easy to construct a malicious test case that causes a hashmap to look through all of the entries in the map. What's more interesting is the average-case performance. When I use Big-O notation in this article, it will refer to average-case performance. The great thing about average-case performance is that you can use expected value for calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## std::unordered_map\n",
    "\n",
    "### The Implementation\n",
    "*std::unordered_map* uses a technique called chaining to build its hashmap. You can think of the underlying structure of *std::unordered_map* as a static array with buckets of linked lists that hold key-value pairs. When a new key-value pair needs to be inserted into the hashmap, *std::unordered_map* takes the key, hashes it, and converts the hash into an index. *std::unordered_map* then goes to its table, finds the index, and checks whether a key or keys already live there. If the index is empty, then the table simply inserts the key-value pair. However, if there are multiple keys, then *std::unordered_map* will check all of the keys that live there already to determine if the key already exists. If it does, then it only changes the value. If it doesn't, then *std::unordered_map* will insert the new key-value pair into a chain of other key-value pairs in the form of a linked-list. When a key-value pair needs to be removed, the map iterates over that bucket's linked list and splices out the key-value pair if it is found.\n",
    "\n",
    "### Theoretical Performance\n",
    "\n",
    "#### Average-Case Behavior\n",
    "\n",
    "Let the total number of buckets (i.e the capacity of the hashmap) be $K$ and let the total number of entries be $N$. Suppose we assume our hash function has a perfectly uniform distribution. That is, suppose that for every possible object, the probability of it hashing to any bucket is the same. Then the probability that an object hashes to a bucket $i$ is $\\frac{1}{K}$ and the expected number of entries per bucket is $N \\cdot \\frac{1}{K} = \\frac{N}{K}$.\n",
    "\n",
    "When we insert, we first have to search every single value in the bucket, so the average-case performance is $O\\left(\\frac{N}{K}\\right)$. This seems like it's linear, but we can say it's amortized O(1) if $\\frac{N}{K} \\approx C$, where $C$ is some constant. Thus, we can achieve amortized O(1) when $N \\approx CK$, which can be maintained by rehashing and expanding the number of buckets to compensate when $N$ gets too high. The time complexity for look-ups and removals is very similar.\n",
    "\n",
    "#### Issues with Chaining\n",
    "Chaining is a very simple method to ensure that keys are safely kept in the bucket they're assigned to. In addition, since we can grow the linked list to infinite length, technically we can never run out of space in our hashmap. The static array does not need to be grown. However, this does come with some significant drawbacks. In the case that elements are mapped to a small range of indices, there's a lot of wasted space in the hashmap. Cache performance is also not great with linked lists, since linked list nodes are not (necessarily) adjacent in memory. As you iterate along the chain, you run into a bunch of cache misses bouncing across memory addresses, which slows down search.\n",
    "\n",
    "Our goal is to minimize wasted space and improve cache performance of searches with the following hashmap implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearMap\n",
    "\n",
    "### The Implementation\n",
    "\n",
    "The first hashmap I implemented uses open addressing with linear probing. The idea of open addressing is that key-value pairs can live in slots that it does not necessarily map to. The underlying structure is an array of objects of type `Entry<K, V>`. Each entry stores the key, the value, the hash value of the key, and some other book keeping items. Suppose you want to insert a key-value pair, and the key hashes to index 21. But in index 21, there already exists a key-value pair. Instead of chaining, what we do is we \"probe\" for an empty slot and stick it in there instead. When we want to look-up a value, we go to the initial bucket that the key hashes to and check if that slot contains the key. If the slot doesn't, then we continue to probe the same way we did looking for an empty slot. \n",
    "\n",
    "Normally, I've seen resources online use tombstones when implementing pure linear open-addressed hashmaps. Tombstones are markers that denote entries that have been used but are now empty since the key value pair has been removed. This makes removals super easy since you just mark them as a tombstone and get on with your day. When you insert values into the hashmap, you can insert them into tombstones. However, when you look up values, you have to skip over tombstones and keep looking because if you stop when you get to a tombstone, there's the possibility that a key that you had added after the deleted key can no longer be reached when probing from the start index.\n",
    "\n",
    "### Theoretical Performance\n",
    "\n",
    "#### Average Case Performance\n",
    "\n",
    "However, like I said, we deal with average-case complexity. To talk about performance with open-addressed maps, we introduce the idea of a probe sequence length, or PSL. The probe sequence length is how far in terms of slots you have to probe \n",
    "\n",
    "#### Benefits of Open Addressing\n",
    "\n",
    "Since all of the entries are placed next to one another in a flat array, this means that cache performance should be better. However, with tombstones, it's possible that interleaving insertions and removals results in longer probe sequences the linked list would have, as mentioned above. According to this [stackoverflow post](https://stackoverflow.com/questions/47781316/linked-list-vs-array-traversal-efficiency), following pointer indirection takes 4 times as many cycles as calculating the next address in an array. \n",
    "\n",
    "Under the assumption of a perfectly uniform hashmap, we previously calculated that there would be $\\frac{N}{K}$ entries in a list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robinhood Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Results\n",
    "\n",
    "### Simple Insertion\n",
    "\n",
    "This test generates a vector of keys from 0 to $K$, shuffles them into a random order, then records the time it takes to insert all of the keys into the hashmap.\n",
    "\n",
    "### Simple Look-Up\n",
    "\n",
    "This test generates a vector of keys from 0 to $K$, shuffles them into a random order, and then inserts the keys into the hashmap. Then it shuffles the keys into a random order again and records the time it takes to look up all of the keys in the vector.\n",
    "\n",
    "### Simple Removal\n",
    "\n",
    "This test generates a vector of keys from 0 to $K$, shuffles them into a random order, then inserts the keys into the hashmap. Then it shuffles the keys into a random order again and records the time it takes to remove all of the keys in the vector.\n",
    "\n",
    "### Insertions with Removals\n",
    "\n",
    "This test generates a vector of keys from 0 to $K$, shuffles them into a random order, then inserts the keys into the hashmap. Then it shuffles half of the randomized keys into a random order again and removes those keys from the hashmap. Finally, all the keys are shuffled into a random order and the test records the time it takes to insert all of the keys in the vector.\n",
    "\n",
    "### Look-Ups with Removals\n",
    "\n",
    "This test generates a vector of keys from 0 to $K$, shuffles them into a random order, then inserts the keys into the hashmap. Then it shuffles half of the randomized keys into a random order again and removes those keys from the hashmap. Finally, all the keys are shuffled into a random order and the test records the time it takes to look-up all of the keys in the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
